# Financial RAG Evaluation Framework

A comprehensive, modular framework for building, tuning, and evaluating Retrieval-Augmented Generation (RAG) systems for financial documents.

## ğŸ“‹ Overview

This framework automates the end-to-end process of:
1. **Processing financial documents** (PDF, CSV, etc.) into structured formats
2. **Generating representative question-answer pairs** for evaluation
3. **Implementing RAG pipelines** with configurable retrievers and generators
4. **Evaluating performance** across multiple retrieval and generation metrics

Whether you're researching RAG techniques, building production systems, or comparing LLM performance, this toolkit provides the infrastructure to rigorously measure and improve results.

## ğŸ” Key Features

- **Complete Processing Pipeline**: Process PDFs and other document formats into RAG-ready formats
- **Automated QA Generation**: Create evaluation datasets without manual annotation
- **BM25 & Vector Retrieval**: Compare traditional and embedding-based approaches
- **Comprehensive Evaluation**: Measures both retrieval accuracy and answer quality
- **LLM Integration**: Works with OpenRouter-compatible APIs (Claude, GPT, etc.)
- **Flexible Configuration**: Easily adjust parameters via config files or CLI

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/LinHuanli/RAG_Eavl.git
cd RAG_Eavl

# Install dependencies
pip install -r requirements.txt
```

### End-to-End Example

Process a financial document, generate QA pairs, run RAG evaluation, and analyze results:

```bash
# 1. Process a financial PDF into structured JSON
python -m main process-document \
    --input_file ./docs/financial_regulation.pdf \
    --output_file ./docs/financial_regulation.json

# 2. Generate QA pairs for evaluation
python -m main generate-qa \
    --docs_path ./docs/financial_regulation.json \
    --output_path ./docs/qa_pairs/evaluation_set.json \
    --num_samples 30

# 3. Run RAG evaluation with default BM25 retriever
python -m main run-rag \
    --docs_path ./docs/financial_regulation.json \
    --test_file_path ./docs/qa_pairs/evaluation_set.json \
    --run_name bm25_baseline \
    --save_output

# 4. Calculate and visualize evaluation metrics
python -m main evaluate \
    --results_file ./outputs/runs/bm25_baseline/rag_results.json \
    --run_name bm25_baseline
```


## ğŸ§© System Components

The framework is organized into modular components:

```
financial_rag_eval/
â”œâ”€â”€ data/               # Document processing components
â”œâ”€â”€ retrieval/          # Document retrieval implementations
â”œâ”€â”€ generation/         # Answer generation and QA components
â”œâ”€â”€ evaluation/         # Metrics and evaluation tools
â”œâ”€â”€ config/             # Configuration management
â”œâ”€â”€ utils/              # Shared utilities
â””â”€â”€ scripts/            # Executable scripts
```

### Data Directory Structure

```
financial-rag-eval/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/            # Original documents (PDFs, etc.)
â”‚   â”œâ”€â”€ processed/      # Processed documents (JSON)
â”‚   â””â”€â”€ qa_pairs/       # Generated QA pairs
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ runs/           # Named experiment runs 
â”‚   â”‚   â”œâ”€â”€ run_20250405_153206/
â”‚   â”‚   â”‚   â”œâ”€â”€ rag_results.json
â”‚   â”‚   â”‚   â””â”€â”€ evaluation_metrics.json
â”‚   â””â”€â”€ reports/        # Analysis reports
â””â”€â”€ logs/
    â””â”€â”€ rag_eval.log
```

## ğŸ“Š Evaluation Metrics

The evaluation framework assesses RAG systems across multiple dimensions:

### Retrieval Accuracy
- **Recall@k**: How many relevant documents are retrieved in the top-k results
- **MRR (Mean Reciprocal Rank)**: How well the system ranks relevant documents

### Answer Quality
- **F1 Score**: Lexical overlap between generated and reference answers
- **Factuality**: LLM-evaluated accuracy of facts in generated answers

## ğŸ› ï¸ Configuration

The system uses a hierarchical configuration approach:

1. **Default Configuration**: Base settings in `config/default_config.yaml`
2. **Custom Config Files**: Override defaults with your own YAML config
3. **Environment Variables**: Set values with `RAG_API_KEY`, etc.
4. **Command Line Arguments**: Highest priority for specific runs

Example of a custom configuration file:

```yaml
# custom_config.yaml
retrieval:
  method: "bm25"
  top_k: 5
  bm25:
    k1: 1.2
    b: 0.75

api:
  model: "anthropic/claude-3.5-sonnet"
  key: "${RAG_API_KEY}"  # Will be replaced from environment
```

## ğŸ“ Advanced Usage

### Comparing Different Retrieval Methods

```bash
# Run with BM25 retrieval
python -m main run-rag \
    --docs_path ./docs/financial_regulation.json \
    --test_file ./docs/qa_pairs/evaluation_set.json \
    --run_name bm25_experiment \
    --config ./configs/bm25_config.yaml

# Run with vector retrieval
python -m main run-rag \
    --docs_path ./docs/financial_regulation.json \
    --test_file ./docs/qa_pairs/evaluation_set.json \
    --run_name vector_experiment \
    --config ./configs/vector_config.yaml

# Compare results
python -m main compare-runs \
    --run1 ./outputs/runs/bm25_experiment \
    --run2 ./outputs/runs/vector_experiment \
    --output ./outputs/reports/retrieval_comparison.pdf
```

### Processing Large Document Collections

For large document sets, use the chunking capability:

```bash
python -m main process-document \
    --input_file ./docs/large_report.pdf \
    --output_file ./docs/large_report_chunked.json \
    --chunk_size 1000 \
    --chunk_overlap 200
```

## ğŸ¤ Contributing

Contributions are welcome!

## ğŸ“„ License

This project is licensed under the Apache-License 2.0 - see the LICENSE file for details.
